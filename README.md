# NLP
 NLP Model Comparision

# üß† Text Classification: Traditional ML vs BERT

This project compares the performance of traditional machine learning models and a transformer-based BERT model on a binary text classification task using the **20 Newsgroups** dataset (specifically the *rec.sport.hockey* and *sci.space* categories).

---

## üîç Problem Statement

The goal is to classify news articles into one of two categories based on their textual content.

---

## üõ†Ô∏è Tools & Libraries

- Python, NumPy, pandas  
- **scikit-learn**: TF-IDF, Naive Bayes, Logistic Regression, SVM, Random Forest  
- **HuggingFace Transformers**: BERT, Trainer API  
- **PyTorch**: Custom Dataset & DataLoader  
- **Matplotlib & seaborn**: For visualizations

---

## üìä Models Compared

| Model               | Accuracy | Precision | Recall | F1-score |
|--------------------|----------|-----------|--------|----------|
| Na√Øve Bayes        | ...      | ...       | ...    | ...      |
| Logistic Regression| ...      | ...       | ...    | ...      |
| SVM (Linear)       | ...      | **üîù**     | **üîù**  | **üîù**    |
| Random Forest      | ...      | ...       | ...    | ...      |
| **BERT (Transformer)** | **üîù**  | N/A       | N/A    | N/A      |

> üîù **Best Accuracy:** BERT  
> ü•á **Best Precision, Recall, F1-score:** SVM

---

## ‚úÖ Key Steps

1. **Data Loading & Cleaning**
   - Loaded 20 Newsgroups dataset from `sklearn.datasets`.
   - Filtered two categories: `rec.sport.hockey` and `sci.space`.
   - Removed headers, footers, and quotes for cleaner text.

2. **TF-IDF Vectorization**
   - Converted text data into numerical format using `TfidfVectorizer` with a 5,000-word vocabulary.

3. **Model Training**
   - Trained and evaluated four classical models:
     - Na√Øve Bayes
     - Logistic Regression
     - Support Vector Machine (SVM)
     - Random Forest
   - Metrics used: Accuracy, Precision, Recall, F1-score

4. **BERT Fine-tuning**
   - Used `bert-base-uncased` from HuggingFace.
   - Tokenized and encoded text data using a custom PyTorch `Dataset` class.
   - Fine-tuned for 0.5 epochs using HuggingFace's `Trainer`.

5. **Result Compilation**
   - Compiled all model scores into a pandas DataFrame for comparison.

---

## üöÄ Future Improvements

- Extend to multi-class classification
- Integrate more transformer models (e.g., RoBERTa, DistilBERT)
- Perform hyperparameter tuning & cross-validation
- Add error analysis & misclassification visualization

---

## üìå Summary

This project demonstrates how transformer-based models like **BERT** outperform classical models in terms of accuracy for text classification. However, **SVM** achieved the highest precision, recall, and F1-score among traditional models ‚Äî proving to be a strong, lightweight alternative.

---

## üì∑ Sample Output

Feel free to include:
- Confusion matrix plots  
- Bar charts comparing model metrics  
- Training logs or screenshots  

---

## ü§ù Contributions

Pull requests, suggestions, and issues are always welcome!

---